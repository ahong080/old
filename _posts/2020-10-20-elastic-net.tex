---
layout: post
title: Elastic net in Scikit-Learn vs. Keras
---

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays

Logistic regression with elastic net regularization is available in both \texttt{sklearn} and \texttt{Keras}. To compare these two approaches, we must be able to set the same hyperparameters for both learning algorithms. However, how to do is not immediately obvious.

The objective function and variable names differ between the original paper and the \texttt{sklearn} documentation. This blog post describes and reconciles these differences. It is assumed the reader understands the purpose of elastic net and the concepts behind regularization.

\par\noindent\rule{\textwidth}{1pt}

The elastic net (\emph{Zou and Hastie. J. R. Statist. Soc. B. 2005}) adds L1 and L2 penalties of lasso and ridge regression methods to the objective function:

$$
L(\lambda_{1}, \lambda_{2}, \beta) = 
\vert y - X\beta \vert^{2}
+ \lambda_{2} \vert \beta \vert^{2}
+ \lambda_{1} \vert \beta \vert_{1}
$$

The model coefficients $\hat{\beta}$ minimize this objective function:

$$
\underset{\beta}{\operatorname{argmin}} \{L(\lambda_{1}, \lambda_{2}, \beta)\}
$$


Elastic net with $\lambda_{2}=0$ is simply ridge regression. Likewise, elastic net with $\lambda_{1}=0$ is simply lasso.

\par\noindent\rule{\textwidth}{1pt}

In the \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html}{\texttt{sklearn} documentation for elastic net}, the objective function to minimize is different:

$$
L = 
\frac{1}{2n} ||y - Xw||^{2}_{2}
+ \alpha \rho ||w||_1
+ \frac{1}{2} \alpha (1 - \rho) ||w||^{2}_{2}
$$

where $\rho$ is \texttt{l1\textunderscore ratio}.

There are several differences:

\begin{enumerate}
    \item Model coefficients are denoted by $w$, not $\beta$.
    \item Norms are denoted with double instead of single lines.
    \item Terms with $L2$ norms are multiplied by $\frac{1}{2}$. This is a mathematical convenience; when the derivative of the term is taken, each of those terms is multiplied by $2$, which cancels the $\frac{1}{2}$. This does not change the solution because 
$ \underset{\beta}{\operatorname{argmin}} \{ L \} = \underset{\beta}{\operatorname{argmin}} \{ kL \} $ for any real value of $k$.
    \item The lasso term (L1 penalty) comes first, whereas in the paper it comes after the ridge regression term (L2 penalty).
    \item $\alpha \rho$ is used instead of $\lambda_{1}$
    \item $\alpha (1-\rho)$ is used instead of $\lambda_{2}$
\end{enumerate}

The documentation describes the equivalence between the L1 and L2 penalty terms, but use different variable names. To make these equations easier to connect to those in the original Zhou and Hastie paper, I set $a=\lambda_{1}$ and $b=\lambda_{2}$ and use the latter notation below:

$$
\alpha = \lambda_{1} + \lambda_{2} \\
\rho = \frac{\lambda_{1}}{\lambda_{2} + \lambda_{2}}\\
$$

The following Python expressions of the above equations will set elastic net hyperparameters $\alpha$ and $\rho$ for elastic net in \texttt{sklearn} using $\lambda_{1}$ and $\lambda_{2}$:


\texttt{alpha = lambda1 + lambda2}

\texttt{\text{l1\textunderscore ratio} = lambda1 / (lambda1 + lambda2)}

\par\noindent\rule{\textwidth}{1pt}

The \href{https://keras.io/api/layers/regularizers/}{\texttt{keras} documentation for elastic net} is minimal. No equation for the objective function is given. The documentation does not even refer to L1 L2 regularization as elastic net. However, the implementation is straightforward; simply use the \texttt{l1\textunderscore l2} regularizer function and set the parameters \texttt{l1} and \texttt{l2} (which are equivalent to $\lambda_{1}$ and $\lambda_{2}$, respectively):

\texttt{tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)}
